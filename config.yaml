# Health Data Pipeline Configuration
# Copy this to config.yaml and fill in your values

# Timezone configuration
timezone:
  # Your home timezone for Strategy A ingestion (HAE CSV/JSON)
  # Used for minute_facts and daily_summary
  default: "America/Los_Angeles"
  
  # Optional: Historical timezone changes (if you moved)
  # changes:
  #   - before: "2025-06-01"
  #     timezone: "America/Los_Angeles"
  #     notes: "Seattle era"
  #   - after: "2025-06-01"
  #     timezone: "America/Chicago"
  #     notes: "Moved to Austin"
  
  # Optional: Known travel periods (for filtering in analysis)
  # travel_periods:
  #   - start: "2025-03-15"
  #     end: "2025-03-22"
  #     timezone: "Europe/London"
  #     notes: "London vacation"

# API configuration
api:
  concept2:
    # Get your token from: https://log.concept2.com/developers
    token: "Ph2KW2lznHKNQibKUWS3BvYQsCNSZqKGPn74tMcZ"
    base_url: "https://log.concept2.com/api/"
  
  # Future APIs can be added here
  # apple_health:
  #   ...

# Data directory paths
data:
  raw_dir: "Data/Raw"
  parquet_dir: "Data/Parquet"
  archive_dir: "Data/Archive"
  error_dir: "Data/Error"

# Ingestion settings
ingestion:
  # Batch size for API pagination
  batch_size: 50
  
  # Retry configuration for API calls
  max_retries: 3
  retry_delay_seconds: 5
  
  # Archive processed files after successful ingestion
  archive_after_ingest: true

# Logging configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "pipeline.log"  # Set to null to disable file logging

# Validation settings
validation:
  # Run validation checks after ingestion
  enabled: true
  
  # Fail on validation errors (vs. just warn)
  strict: false

# ============================================================================
# Google Drive Sources
# ============================================================================
# Defines *what* to fetch. Parent folder IDs are set in your .env file.
# The `config.py` script loads the IDs from .env and injects them.
# ============================================================================
drive_sources:
  
  labs:
    # 'single_file': Downloads one file by name from a parent folder.
    type: "single_file"
    file_name: "AllLabsHistory.xlsx"
    parent_folder_id: "" # Populated by HDP_LABS_FOLDER_ID from .env
    output_path: "Data/Raw/labs/labs-master-latest.xlsx"
    sheet_name: "Lab Results" # For the ingestion step

  protocols:
    type: "single_file"
    # Find by name. Set HDP_PROTOCOLS_FOLDER_ID in .env
    file_name: "CompoundMasterLog.xlsx"
    parent_folder_id: "" # Populated by HDP_PROTOCOLS_FOLDER_ID from .env
    output_path: "Data/Raw/labs/protocols-master-latest.xlsx"
    sheet_name: "Events" # For the ingestion step

  hae_csv:
    # 'folder_sync': Downloads ALL files from a subfolder (by name).
    type: "folder_sync"
    # Set HDP_HAE_PARENT_ID in .env
    parent_folder_id: "" # Populated by HDP_HAE_PARENT_ID from .env
    folder_name: "Daily" # The subfolder to sync from
    output_dir: "Data/Raw/HAE/CSV"
    # (Optional) Name of a subfolder *inside* "Daily" to move files to after sync
    archive_subfolder_name: "Archive" 

  hae_json:
    type: "folder_sync"
    parent_folder_id: "" # Populated by HDP_HAE_PARENT_ID from .env
    folder_name: "Workouts"
    output_dir: "Data/Raw/HAE/JSON"
    archive_subfolder_name: "Archive"

  hae_quick:
    type: "folder_sync"
    parent_folder_id: "" # Populated by HDP_HAE_PARENT_ID from .env
    folder_name: "Quick"
    output_dir: "Data/Raw/HAE/Quick"
    archive_subfolder_name: "Archive"